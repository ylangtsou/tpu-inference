import tempfile

import jax
import pytest
import torch
import torchax
import utils as test_utils
from jax._src import test_util as jtu
from jax.sharding import NamedSharding, PartitionSpec
from torchax.interop import torch_view
from torchax.ops.mappings import j2t, t2j
from vllm.config import ParallelConfig, set_current_vllm_config
from vllm.distributed.parallel_state import (ensure_model_parallel_initialized,
                                             init_distributed_environment)
from vllm.engine.arg_utils import EngineArgs
from vllm.forward_context import set_forward_context
from vllm.model_executor.layers.fused_moe.layer import FusedMoE

from tpu_inference.layers.common.quantization import quantize_to_mxfp4_packed
from tpu_inference.layers.vllm.quantization import get_tpu_quantization_config
from tpu_inference.layers.vllm.quantization.mxfp4 import (VllmMxfp4Config,
                                                          VllmMxfp4MoEMethod)

P = PartitionSpec
MODELS = ["openai/gpt-oss-20b"]

if not jtu.is_device_tpu_at_least(version=7):
    pytest.skip(allow_module_level=True, reason="Expected TPUv7+")


@pytest.fixture(autouse=True)
def setup_environment():
    # This is a fake config used for init dist env.
    # RowParallelLinear needs dist env to be initialized.
    engine_args = EngineArgs(
        model=MODELS[0],
        max_model_len=64,
        max_num_batched_tokens=64,
        max_num_seqs=4,
        load_format='dummy',
    )

    vllm_config = engine_args.create_engine_config()

    with set_current_vllm_config(vllm_config):
        temp_file = tempfile.mkstemp()[1]
        init_distributed_environment(
            1,
            0,
            local_rank=0,
            distributed_init_method=f"file://{temp_file}",
            backend="gloo")
        ensure_model_parallel_initialized(1, 1)


@pytest.mark.parametrize("model", MODELS)
@pytest.mark.parametrize("mesh", [
    test_utils.get_spmd_mesh(1),
    test_utils.get_spmd_mesh(jax.local_device_count())
])
def test_quant_override(model, mesh):

    engine_args = EngineArgs(
        model=model,
        max_model_len=64,
        max_num_batched_tokens=64,
        max_num_seqs=4,
        load_format='dummy',
    )
    vllm_config = engine_args.create_engine_config()
    vllm_config.model_config.dtype = torch.bfloat16

    quant_config = get_tpu_quantization_config(vllm_config, mesh)
    assert isinstance(quant_config, VllmMxfp4Config)
    assert quant_config.vllm_config == vllm_config
    assert quant_config.mesh == mesh


@pytest.mark.parametrize("mesh", [
    test_utils.get_spmd_mesh(1),
    test_utils.get_spmd_mesh(jax.local_device_count())
])
@pytest.mark.parametrize("num_tokens", [8])
@pytest.mark.parametrize("intermediate_size", [1024])
@pytest.mark.parametrize("hidden_size", [128])
@pytest.mark.parametrize("num_experts", [8])
@pytest.mark.parametrize("topk", [2])
def test_mxfp4_fused_moe(mesh, num_tokens, intermediate_size, hidden_size,
                         num_experts, topk):
    torch.manual_seed(42)
    dtype = torch.bfloat16

    a = torch.randn((num_tokens, hidden_size), dtype=dtype) / 10
    w1 = torch.randn(
        (num_experts, 2 * intermediate_size, hidden_size), dtype=dtype) / 10
    w2 = torch.randn(
        (num_experts, hidden_size, intermediate_size), dtype=dtype) / 10
    w1_weight, w1_weight_scale = j2t(quantize_to_mxfp4_packed(t2j(w1)))
    w2_weight, w2_weight_scale = j2t(quantize_to_mxfp4_packed(t2j(w2)))

    w1_bias = torch.randn(
        (num_experts, 2 * intermediate_size), dtype=dtype) / 10
    w2_bias = torch.randn((num_experts, hidden_size), dtype=dtype) / 10
    score = torch.randn((num_tokens, num_experts), dtype=dtype)

    engine_args = EngineArgs(
        model=MODELS[0],
        max_model_len=64,
        max_num_batched_tokens=64,
        max_num_seqs=4,
        load_format='dummy',
    )
    vllm_config = engine_args.create_engine_config()
    vllm_config.model_config.dtype = dtype

    quant_config = get_tpu_quantization_config(vllm_config, mesh)
    with set_current_vllm_config(vllm_config):
        vllm_fused_moe = FusedMoE(
            num_experts=num_experts,
            top_k=topk,
            hidden_size=hidden_size,
            intermediate_size=intermediate_size,
            reduce_results=False,
            renormalize=False,
            tp_size=1,
            dp_size=1,
            quant_config=quant_config,
            has_bias=True,
        )
    vllm_fused_moe.w13_weight.data = w1_weight
    vllm_fused_moe.w2_weight.data = w2_weight
    vllm_fused_moe.w13_weight_scale.data = w1_weight_scale
    vllm_fused_moe.w2_weight_scale.data = w2_weight_scale
    vllm_fused_moe.w13_bias.data = w1_bias
    vllm_fused_moe.w2_bias.data = w2_bias

    with torchax.default_env(), set_forward_context(None, vllm_config):
        assert isinstance(vllm_fused_moe.quant_method, VllmMxfp4MoEMethod)

        jax_a = a.to('jax')
        jax_a.apply_jax_(jax.device_put, NamedSharding(mesh, P(None, None)))
        score = torch_view(t2j(score))
        score.apply_jax_(jax.device_put, NamedSharding(mesh, P(None, None)))

        vllm_fused_moe.quant_method.process_weights_after_loading(
            vllm_fused_moe)

        # Because we are dequantizing mxfp4 weights for now, we verify if
        # dequantized weights matches with the original weights.
        # Due to NaN, comparing two values are difficult. Therefore, we utilize
        # nanmean instead.
        torch.testing.assert_close(torch.nanmean(vllm_fused_moe.w13_weight),
                                   torch.nanmean(w1),
                                   check_device=False,
                                   equal_nan=True,
                                   rtol=0.2,
                                   atol=0.1)
        torch.testing.assert_close(torch.nanmean(vllm_fused_moe.w2_weight),
                                   torch.nanmean(w2),
                                   check_device=False,
                                   equal_nan=True,
                                   rtol=0.2,
                                   atol=0.1)

        vllm_fused_moe(jax_a, score)


@pytest.mark.parametrize("mesh", [
    test_utils.get_spmd_mesh(1),
    test_utils.get_spmd_mesh(jax.local_device_count())
])
@pytest.mark.parametrize("num_tokens", [8])
@pytest.mark.parametrize("intermediate_size", [1024])
@pytest.mark.parametrize("hidden_size", [128])
@pytest.mark.parametrize("num_experts", [8])
@pytest.mark.parametrize("topk", [2])
def test_mxfp4_fused_moe_use_kernel(mesh, num_tokens, intermediate_size,
                                    hidden_size, num_experts, topk):
    torch.manual_seed(42)
    dtype = torch.bfloat16

    a = torch.randn((num_tokens, hidden_size), dtype=dtype) / 10
    w1 = torch.randn(
        (num_experts, 2 * intermediate_size, hidden_size), dtype=dtype) / 10
    w2 = torch.randn(
        (num_experts, hidden_size, intermediate_size), dtype=dtype) / 10
    w1_weight, w1_weight_scale = j2t(quantize_to_mxfp4_packed(t2j(w1)))
    w2_weight, w2_weight_scale = j2t(quantize_to_mxfp4_packed(t2j(w2)))

    w1_bias = torch.randn(
        (num_experts, 2 * intermediate_size), dtype=dtype) / 10
    w2_bias = torch.randn((num_experts, hidden_size), dtype=dtype) / 10
    score = torch.randn((num_tokens, num_experts), dtype=dtype)

    engine_args = EngineArgs(
        model=MODELS[0],
        max_model_len=64,
        max_num_batched_tokens=64,
        max_num_seqs=4,
        load_format='dummy',
    )
    vllm_config = engine_args.create_engine_config()
    vllm_config.model_config.dtype = dtype
    vllm_config.parallel_config = ParallelConfig(
        tensor_parallel_size=mesh.devices.size, enable_expert_paralle=True)

    quant_config = get_tpu_quantization_config(vllm_config, mesh)
    with set_current_vllm_config(vllm_config):
        vllm_fused_moe = FusedMoE(
            num_experts=num_experts,
            top_k=topk,
            hidden_size=hidden_size,
            intermediate_size=intermediate_size,
            reduce_results=False,
            renormalize=False,
            tp_size=1,
            dp_size=1,
            quant_config=quant_config,
            has_bias=True,
        )
    vllm_fused_moe.w13_weight.data = w1_weight
    vllm_fused_moe.w2_weight.data = w2_weight
    vllm_fused_moe.w13_weight_scale.data = w1_weight_scale
    vllm_fused_moe.w2_weight_scale.data = w2_weight_scale
    vllm_fused_moe.w13_bias.data = w1_bias
    vllm_fused_moe.w2_bias.data = w2_bias

    with torchax.default_env(), set_forward_context(None, vllm_config):
        assert isinstance(vllm_fused_moe.quant_method, VllmMxfp4MoEMethod)

        jax_a = a.to('jax')
        jax_a.apply_jax_(jax.device_put, NamedSharding(mesh, P(None, None)))
        score = torch_view(t2j(score))
        score.apply_jax_(jax.device_put, NamedSharding(mesh, P(None, None)))

        vllm_fused_moe.quant_method.use_kernel = True
        vllm_fused_moe.quant_method.process_weights_after_loading(
            vllm_fused_moe)
        vllm_fused_moe.quant_method.block_size = {
            "bt": 32,
            "bf": 512,
            "bd1": 512,
            "bd2": 512,
            "btc": 32,
            "bfc": 256,
            "bd1c": 256,
            "bd2c": 256,
        }

        vllm_fused_moe(jax_a, score)
